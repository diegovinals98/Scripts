{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "console.log(\"IMportaciones hechas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Direccion de contenido \n",
    "file_path = ('./ai-medical-chatbot.csv')\n",
    "\n",
    "#file_path = \"./ai-medical-chatbot.csv\"\n",
    "conversations_df = pd.read_csv(file_path)\n",
    "print(conversations_df)\n",
    "\n",
    "# Preparar las conversaciones\n",
    "conversations = list(zip(conversations_df['Description'], conversations_df['Doctor']))\n",
    "\n",
    "# Definir la clase para el dataset\n",
    "class MedicalChatDataset(Dataset):\n",
    "    def __init__(self, conversations, tokenizer):\n",
    "        self.conversations = conversations\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.conversations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        question, answer = self.conversations[idx]\n",
    "        question_tokens = self.tokenizer(question)\n",
    "        answer_tokens = self.tokenizer(answer)\n",
    "        return torch.tensor(question_tokens), torch.tensor(answer_tokens)\n",
    "\n",
    "# Función para hacer padding a las secuencias\n",
    "def pad_sequences(batch):\n",
    "    # Obtener las longitudes de todas las secuencias en el batch\n",
    "    question_lengths = [len(x[0]) for x in batch]\n",
    "    answer_lengths = [len(x[1]) for x in batch]\n",
    "    \n",
    "    # Obtener la longitud máxima para padding\n",
    "    max_question_len = max(question_lengths)\n",
    "    max_answer_len = max(answer_lengths)\n",
    "    \n",
    "    # Hacer padding en las preguntas y respuestas\n",
    "    padded_questions = [torch.cat([q, torch.zeros(max_question_len - len(q))]) for q, _ in batch]\n",
    "    padded_answers = [torch.cat([a, torch.zeros(max_answer_len - len(a))]) for _, a in batch]\n",
    "    \n",
    "    # Convertir a tensores\n",
    "    padded_questions = torch.stack(padded_questions)\n",
    "    padded_answers = torch.stack(padded_answers)\n",
    "    \n",
    "    return padded_questions.long(), padded_answers.long()\n",
    "\n",
    "# Definir la arquitectura de la red LSTM\n",
    "class MedicalChatbot(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(MedicalChatbot, self).__init__()\n",
    "        \n",
    "        # Capa de embedding para convertir palabras en vectores\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Capa LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        # Capa totalmente conectada para generar la salida\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        output = self.fc(lstm_out)  # Generar una salida para cada paso de la secuencia\n",
    "        return output\n",
    "\n",
    "# Función de entrenamiento\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            # Pasar las entradas por el modelo\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Ajustar los targets para todas las palabras de la secuencia\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "            targets = targets.view(-1)\n",
    "            \n",
    "            # Asegurarse de que los tamaños coincidan\n",
    "            if outputs.size(0) != targets.size(0):\n",
    "                min_size = min(outputs.size(0), targets.size(0))\n",
    "                outputs = outputs[:min_size]\n",
    "                targets = targets[:min_size]\n",
    "            \n",
    "            # Calcular la pérdida\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader)}')\n",
    "\n",
    "# Definir el tokenizador mejorado con control de rangos\n",
    "def simple_tokenizer(text, vocab_size=128):\n",
    "    tokenized = []\n",
    "    for c in text:\n",
    "        if ord(c) < vocab_size:\n",
    "            tokenized.append(ord(c))  # Usar el valor ASCII\n",
    "        else:\n",
    "            tokenized.append(0)  # Asignar un índice de \"<UNK>\" para caracteres fuera del rango\n",
    "    return tokenized\n",
    "\n",
    "# Parámetros del modelo\n",
    "vocab_size = 128  # Mantener el vocabulario de caracteres ASCII básicos\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "output_dim = vocab_size  # Para generar una secuencia de caracteres ASCII\n",
    "num_layers = 2\n",
    "num_epochs = 100\n",
    "batch_size = 2\n",
    "\n",
    "# Crear dataset y dataloader con collate_fn para padding\n",
    "tokenizer = lambda text: simple_tokenizer(text, vocab_size)\n",
    "dataset = MedicalChatDataset(conversations, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_sequences)\n",
    "\n",
    "# Crear el modelo\n",
    "model = MedicalChatbot(vocab_size, embedding_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Definir la función de pérdida y el optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Entrenar el modelo\n",
    "train_model(model, dataloader, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Función para predecir la respuesta\n",
    "def predict_response(model, question, tokenizer, vocab_size):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Tokenizar la pregunta y convertirla a tensor\n",
    "        question_tokens = tokenizer(question)\n",
    "        question_tensor = torch.tensor(question_tokens).unsqueeze(0)\n",
    "        \n",
    "        # Pasar la pregunta por el modelo\n",
    "        output = model(question_tensor)\n",
    "        \n",
    "        # Obtener los índices de los tokens con la mayor probabilidad para cada paso de la secuencia\n",
    "        predicted_token_idxs = torch.argmax(output, dim=2).squeeze().tolist()\n",
    "        \n",
    "        # Convertir los índices de vuelta a caracteres ASCII\n",
    "        predicted_chars = [chr(idx) if idx < vocab_size else \"<UNK>\" for idx in predicted_token_idxs]\n",
    "        \n",
    "        return ''.join(predicted_chars)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
